<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Your awesome title</title>
    <description>Working notes on data mining and early book history.</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 12 Nov 2015 21:57:19 -0500</pubDate>
    <lastBuildDate>Thu, 12 Nov 2015 21:57:19 -0500</lastBuildDate>
    <generator>Jekyll v3.0.0</generator>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Tom&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &#39;Hi, Tom&#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Thu, 05 Nov 2015 10:35:55 -0500</pubDate>
        <link>http://yourdomain.com/posts/welcome-to-jekyll</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/welcome-to-jekyll</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Clustering Semantic Vectors</title>
        <description>&lt;p&gt;Google’s Word2Vec and Stanford’s GloVe have recently offered two fantastic open source software packages capable of transposing words into a high dimension vector space. In both cases, a vector’s position within the high dimensional space gives a good indication of the word’s semantic class (among other things), and in both cases these vector positions can be used in a variety of applications. In the post below, I’ll discuss one approach you can take to clustering the vectors into coherent semantic groupings.&lt;/p&gt;

&lt;p&gt;Both Word2Vec and GloVe can create vector spaces given a large training corpus, but both maintain pretrained vectors as well. To get started with ~1GB of pretrained vectors from GloVe, one need only run the following lines:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;wget http://www-nlp.stanford.edu/data/glove.6B.300d.txt.gz
gunzip glove.6B.300d.txt.gz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you unzip and then glance at glove.6B.300d.txt, you’ll see that it’s organized as follows:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;the 0.04656 0.21318 -0.0074364 [...] 0.053913
, -0.25539 -0.25723 0.13169 [...] 0.35499
. -0.12559 0.01363 0.10306 [...] 0.13684
of -0.076947 -0.021211 0.21271 [...] -0.046533
to -0.25756 -0.057132 -0.6719 [...] -0.070621
[...]
sandberger 0.429191 -0.296897 0.15011 [...] -0.0590532&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Each new line contains a token followed by 300 signed floats, and those values appear to be organized from most to least common. Given this ready format, it’s fairly straightforward to get straight to clustering!&lt;/p&gt;

&lt;p&gt;There are a variety of methods for clustering vectors, including density-based clustering, hierarchical clustering, and centroid clustering. One of the most intuitive and most commonly used centroid-based methods is K-Means. Given a collection of points in a space, K-Means uses a Hunger Games style random lottery to pick a few lucky points (colored green below), then assigns each of the non-lucky points to the lucky point to which it’s closest. Using these preliminary groupings, the next step is to find the “centroid” (or geometric center) of each group, using the same technique one would use to find the center of a square. These centroids become the new lucky points, and again each non-lucky point is again assigned to the lucky point to which it’s closest. This process continues until the centroids settle down and stop moving, after which the clustering is complete. Here’s a nice visual description of K-Means &lt;a href=&quot;http://shabal.in/visuals.html&quot;&gt;source&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post_images/clustering_semantic_vectors/kmeans.gif&quot; id=&quot;gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To cluster the GloVe vectors in a similar fashion, one can use the sklearn package in Python, along with a few other packages:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;from __future__ import division
from sklearn.cluster import KMeans 
from numbers import Number
from pandas import DataFrame
import sys, codecs, numpy&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It will also be helpful to build a class to mimic the behavior of autovivification in Perl, which is essentially the process of creating new default hash values given a new key. In Python, this behavior is available through collections.defaultdict(), but the latter isn’t serializable, so the following class is handy. Given an input key it hasn’t seen, the class will create an empty list as the corresponding hash value:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;class autovivify_list(dict):
    &amp;#39;&amp;#39;&amp;#39;A pickleable version of collections.defaultdict&amp;#39;&amp;#39;&amp;#39;
    def __missing__(self, key):
        &amp;#39;&amp;#39;&amp;#39;Given a missing key, set initial value to an empty list&amp;#39;&amp;#39;&amp;#39;
        value = self[key] = []
        return value

    def __add__(self, x):
        &amp;#39;&amp;#39;&amp;#39;Override addition for numeric types when self is empty&amp;#39;&amp;#39;&amp;#39;
        if not self and isinstance(x, Number):
            return x
        raise ValueError

    def __sub__(self, x):
        &amp;#39;&amp;#39;&amp;#39;Also provide subtraction method&amp;#39;&amp;#39;&amp;#39;
        if not self and isinstance(x, Number):
            return -1 * x
        raise ValueError&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We also want a method to read in a vector file (e.g. glove.6B.300d.txt) and store each word and the position of that word within the vector space. Because reading in and analyzing some of the larger GloVe files can take a long time, to get going quickly one can limit the number of lines to read from the input file by specifying a global value (n_words), which is defined later on:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;def build_word_vector_matrix(vector_file, n_words):
    &amp;#39;&amp;#39;&amp;#39;Return the vectors and labels for the first n_words in vector file&amp;#39;&amp;#39;&amp;#39;
    numpy_arrays = []
    labels_array = []
    with codecs.open(vector_file, &amp;#39;r&amp;#39;, &amp;#39;utf-8&amp;#39;) as f:
        for c, r in enumerate(f):
            sr = r.split()
            labels_array.append(sr[0])
            numpy_arrays.append( numpy.array([float(i) for i in sr[1:]]) )

            if c == n_words:
                return numpy.array( numpy_arrays ), labels_array

    return numpy.array( numpy_arrays ), labels_array&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Scikit-Learn’s implementation of K-Means returns an object (cluster_labels in these snippets) that indicates the cluster to which each input vector belongs. That object doesn’t tell one which word belongs in each cluster, however, so the following method takes care of this. Because all of the words being analyzed are stored in labels_array and the cluster to which each word belongs is stored in cluster_labels, the following method can easily map those two sequences together:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;def find_word_clusters(labels_array, cluster_labels):
    &amp;#39;&amp;#39;&amp;#39;Return the set of words in each cluster&amp;#39;&amp;#39;&amp;#39;
    cluster_to_words = autovivify_list()
    for c, i in enumerate(cluster_labels):
        cluster_to_words[ i ].append( labels_array[c] )
    return cluster_to_words&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we can call the methods above, perform K-Means clustering, and print the contents of each cluster with the following block:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;if __name__ == &amp;quot;__main__&amp;quot;:
    input_vector_file = sys.argv[1] # Vector file input (e.g. glove.6B.300d.txt)
    n_words           = int(sys.argv[2]) # Number of words to analyze 
    reduction_factor  = float(sys.argv[3]) # Amount of dimension reduction {0,1}
    clusters_to_make  = int( n_words * reduction_factor ) # Number of clusters to make
    df, labels_array  = build_word_vector_matrix(input_vector_file, n_words)
    kmeans_model      = KMeans(init=&amp;#39;k-means++&amp;#39;, n_clusters=clusters_to_make, n_init=10)
    kmeans_model.fit(df)

    cluster_labels    = kmeans_model.labels_
    cluster_inertia   = kmeans_model.inertia_
    cluster_to_words  = find_word_clusters(labels_array, cluster_labels)

    for c in cluster_to_words:
        print cluster_to_words[c]
        print &amp;quot;\n&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The full script is available &lt;a href=&quot;https://gist.github.com/duhaime/9330473f9a4e288f00af&quot;&gt;here&lt;/a&gt;. To run it, one needs to specify the vector file to be read in, the number of words one wishes to sample from that file (one can of course read them all, but doing so can take some time), and the “reduction factor”, which determines the number of clusters to be made. If one specifies a reduction factor of .1, for instance, the routine will produce n*.1 clusters, where n is the number of words sampled from the file. The following command reads in the first 10,000 words, and produces 1,000 clusters:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;python cluster_vectors.py glove.6B.300d.txt 10000 .1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The output of this command is the series of clusters produced by the K-Means clustering:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;[u&amp;#39;Chicago&amp;#39;, u&amp;#39;Boston&amp;#39;, u&amp;#39;Houston&amp;#39;, u&amp;#39;Atlanta&amp;#39;, u&amp;#39;Dallas&amp;#39;, u&amp;#39;Denver&amp;#39;, u&amp;#39;Philadelphia&amp;#39;, u&amp;#39;Baltimore&amp;#39;, u&amp;#39;Cleveland&amp;#39;, u&amp;#39;Pittsburgh&amp;#39;, u&amp;#39;Buffalo&amp;#39;, u&amp;#39;Cincinnati&amp;#39;, u&amp;#39;Louisville&amp;#39;, u&amp;#39;Milwaukee&amp;#39;, u&amp;#39;Memphis&amp;#39;, u&amp;#39;Indianapolis&amp;#39;, u&amp;#39;Auburn&amp;#39;, u&amp;#39;Dame&amp;#39;]

[u&amp;#39;Product&amp;#39;, u&amp;#39;Products&amp;#39;, u&amp;#39;Shipping&amp;#39;, u&amp;#39;Brand&amp;#39;, u&amp;#39;Customer&amp;#39;, u&amp;#39;Items&amp;#39;, u&amp;#39;Retail&amp;#39;, u&amp;#39;Manufacturer&amp;#39;, u&amp;#39;Supply&amp;#39;, u&amp;#39;Cart&amp;#39;, u&amp;#39;SKU&amp;#39;, u&amp;#39;Hardware&amp;#39;, u&amp;#39;OEM&amp;#39;, u&amp;#39;Warranty&amp;#39;, u&amp;#39;Brands&amp;#39;]

[u&amp;#39;home&amp;#39;, u&amp;#39;house&amp;#39;, u&amp;#39;homes&amp;#39;, u&amp;#39;houses&amp;#39;, u&amp;#39;housing&amp;#39;, u&amp;#39;offices&amp;#39;, u&amp;#39;household&amp;#39;, u&amp;#39;acres&amp;#39;, u&amp;#39;residence&amp;#39;]

[...]

[u&amp;#39;Night&amp;#39;, u&amp;#39;Disney&amp;#39;, u&amp;#39;Magic&amp;#39;, u&amp;#39;Dream&amp;#39;, u&amp;#39;Ultimate&amp;#39;, u&amp;#39;Fantasy&amp;#39;, u&amp;#39;Theme&amp;#39;, u&amp;#39;Adventure&amp;#39;, u&amp;#39;Cruise&amp;#39;, u&amp;#39;Potter&amp;#39;, u&amp;#39;Angels&amp;#39;, u&amp;#39;Adventures&amp;#39;, u&amp;#39;Dreams&amp;#39;, u&amp;#39;Wonder&amp;#39;, u&amp;#39;Romance&amp;#39;, u&amp;#39;Mystery&amp;#39;, u&amp;#39;Quest&amp;#39;, u&amp;#39;Sonic&amp;#39;, u&amp;#39;Nights&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I’m currently using these word clusters for fuzzy plagiarism detection, but they can serve a wide variety of purposes. If you find them helpful for a project you’re working on, feel free to drop me a note below!&lt;/p&gt;

</description>
        <pubDate>Sat, 12 Sep 2015 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/clustering-semantic-vectors</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/clustering-semantic-vectors</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
