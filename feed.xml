<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Douglas Duhaime</title>
    <description>Working notes on data mining and early book history.</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 14 Feb 2016 10:15:11 -0500</pubDate>
    <lastBuildDate>Sun, 14 Feb 2016 10:15:11 -0500</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Plagiary Poets</title>
        <description>&lt;p&gt;Last week I launched &lt;a href=&quot;http://plagiarypoets.io&quot;&gt;PlagiaryPoets.io&lt;/a&gt;, an interactive app that visualizes text reuse within early poetry:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://plagiarypoets.io&quot;&gt;
  &lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/plagiary_poets/plagiary_poets.png&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I was inspired to build the app after spending a month studying D3.js with &lt;a href=&quot;https://github.com/bobholt&quot;&gt;Bob Holt&lt;/a&gt;, &lt;a href=&quot;https://github.com/jugglinmike&quot;&gt;Mike Pennisi&lt;/a&gt;, and &lt;a href=&quot;http://yannickassogba.info/&quot;&gt;Yannick Assogba&lt;/a&gt;, three brilliant developers who work for &lt;a href=&quot;https://bocoup.com/&quot;&gt;Bocoup&lt;/a&gt;. The site was a lot of fun to build, and I look forward to launching related projects in the months to come. If you have any thoughts about this one, feel free to drop me a line below!&lt;/p&gt;

</description>
        <pubDate>Sat, 06 Feb 2016 10:24:24 -0500</pubDate>
        <link>http://yourdomain.com/posts/plagiary-poets</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/plagiary-poets</guid>
        
        
        <category>digital-humanities</category>
        
        <category>text-reuse</category>
        
        <category>d3js</category>
        
        <category>poetry</category>
        
      </item>
    
      <item>
        <title>Visualizing Shakespearean Characters</title>
        <description>&lt;script src=&quot;//d3js.org/d3.v3.min.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://s3.amazonaws.com/duhaime-shakespeare/js/jquery-1.7.2.min.js&quot;&gt;&lt;/script&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/shakes_characters.css&quot; /&gt;

&lt;p&gt;Some time ago, I was intrigued to discover that Shakespeare’s Histories have a noticeable lack of female characters &lt;a href=&quot;http://douglasduhaime.com/blog/classifying-shakespearean-drama-with-sparse-feature-sets&quot;&gt;[link]&lt;/a&gt;. Since then, I’ve been curious to further explore the nuances of Shakespearean characters, paying particular respect to the gender dynamics of the Bard’s plays. This post is a quick sketch of some of the insights to which that curiosity has led.&lt;/p&gt;

&lt;p&gt;To get a closer look at Shakespeare’s characters, I ran some analysis on the Folger Shakespeare Library’s gold-standard set of Shakespearean texts &lt;a href=&quot;http://www.folgerdigitaltexts.org/&quot;&gt;[link]&lt;/a&gt;, all of which are encoded in fantastic XML markup that captures a number of character-level attributes, including gender. Using that markup, I extracted data for each character in Shakespeare’s plays, and then scoured through those features in search of patterns. All of the characters with an identified gender in this dataset are plotted below (mouseover for character name and source play):&lt;/p&gt;

&lt;!-- Words Spoken by Character Entrance Plot --&gt;
&lt;div id=&quot;characterWords&quot;&gt;&lt;/div&gt;
&lt;script src=&quot;/js/shakespearean_characters.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Looking at this plot, we can see that the most prominent characters in Shakespearean drama are almost all well-known, titular males. There is also a noticeable inverse-relationship between a character’s prominence and the point in the play wherein that character is introduced. Looking more closely at the plot, I’ve further noticed that Shakespeare was curiously consistent in his treatment of characters who appear in multiple plays. In both &lt;i&gt;1 Henry IV&lt;/i&gt; and &lt;i&gt;2 Henry IV&lt;/i&gt;, for instance, Falstaff is given ~6,000 words and is introduced only a few hundred words into the work. Looking at the long tail, by contrast, one finds that among the outspoken characters introduced after the ~15,000 word mark—including Westmoreland and Bedford in &lt;i&gt;2 Henry IV&lt;/i&gt;, and Cade, Clifford, and Iden from &lt;i&gt;2 Henry VI&lt;/i&gt;—nearly all hail from Histories.&lt;/p&gt;

&lt;p&gt;While the plot above gives one a birds’ eye view of Shakespeare’s characters, the plot doesn’t make it particularly easy to differentiate male and female character dynamics. As a step in this direction, the plot below visualizes character entrances by gender for each of Shakespeare’s plays:&lt;/p&gt;

&lt;!-- first and last entrance by gender plot --&gt;
&lt;div id=&quot;minMaxEntrance&quot;&gt;&lt;/div&gt;
&lt;script async=&quot;&quot; src=&quot;js/min_max_entrances.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Examining at the distribution along the x-axis, we can see that male characters consistently enter the stage before female characters. An exception to this general rule may be found in the Comedies, as plays like &lt;i&gt;Taming of the Shrew&lt;/i&gt;, &lt;i&gt;All’s Well that Ends Well&lt;/i&gt;, and &lt;i&gt;Midsummer Nights’ Dream&lt;/i&gt; begin with female characters on stage. Looking at the distribution along the y-axis, we can also see that for most plays, male characters continue to be introduced on stage long after the last female characters have been introduced.&lt;/p&gt;

&lt;p&gt;Given the plots above, some might conclude that Shakespeare privileged male characters over female characters, as he introduced the former earlier and tended to give them more lines. There is evidence in the plays, however, that points in the opposite direction. Looking at Shakespeare’s minor characters, we see that the smallest and least significant roles in each play were almost universally assigned to males:&lt;/p&gt;

&lt;!-- min and max words by gender plot --&gt;
&lt;div id=&quot;minMaxWords&quot;&gt;&lt;/div&gt;
&lt;script async=&quot;&quot; src=&quot;js/min_max_words.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Here we see that even important males characters such as Fleance in &lt;i&gt;Macbeth&lt;/i&gt; and Cornelius in &lt;i&gt;Hamlet&lt;/i&gt; are given very few lines indeed, and the smallest female roles are consistently given more lines than the smallest male roles.&lt;/p&gt;

&lt;p&gt;In sum, the plots above show that a number of heretofore undisclosed patterns emerge when we analyze Shakespeare’s characters in the aggregate. However, the plots above don’t show the connections between characters. One way to investigate these interconnections is through a co-occurrence matrix, in which each cell represents the degree to which two characters appear on stage concurrently:&lt;/p&gt;

&lt;!-- character cooccurrence plot --&gt;
&lt;aside class=&quot;selection-menu&quot; style=&quot;margin-top:20px;&quot;&gt;
    &lt;p&gt;Play:
        &lt;select class=&quot;play-menu form-group select-wrapper form-control&quot; id=&quot;selected_json&quot;&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/1H4.json&#39;&quot;&gt;Henry_IV_i&lt;/option&gt; 
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Ant.json&#39;&quot;&gt;Antony_And_Cleopatra&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/MND.json&#39;&quot;&gt;Midsummer-Nights_Dream&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/AWW.json&#39;&quot;&gt;Alls_Well&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Cor.json&#39;&quot;&gt;Coriolanus&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Cym.json&#39;&quot;&gt;Cymbeline&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Ham.json&#39;&quot;&gt;Hamlet&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/JC.json&#39;&quot;&gt;Julius_Caesar&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Lr.json&#39;&quot;&gt;King_Lear&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/LLL.json&#39;&quot;&gt;Loves_Labours_Lost&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Mac.json&#39;&quot;&gt;Macbeth&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/MM.json&#39;&quot;&gt;Measure_For_Measure&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Ado.json&#39;&quot;&gt;Much_Ado&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Oth.json&#39;&quot;&gt;Othello&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Per.json&#39;&quot;&gt;Pericles&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Rom.json&#39;&quot;&gt;Romeo_And_Juliet&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Err.json&#39;&quot;&gt;Comedy_Of_Errors&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Jn.json&#39;&quot;&gt;King_John&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/MV.json&#39;&quot;&gt;Merchant_Of_Venice&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Wiv.json&#39;&quot;&gt;Merry_Wives_Of_Windsor&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Shr.json&#39;&quot;&gt;Taming_Of_The_Shrew&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Tmp.json&#39;&quot;&gt;Tempest&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/TGV.json&#39;&quot;&gt;Two_Gentlemen_Of_Verona&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/TNK.json&#39;&quot;&gt;Two_Noble_Kinsmen&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/WT.json&#39;&quot;&gt;Winters_Tale&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Tim.json&#39;&quot;&gt;Timon_Of_Athens&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Tit.json&#39;&quot;&gt;Titus_Andronicus&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/Tro.json&#39;&quot;&gt;Troilus_And_Cressida&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/TN.json&#39;&quot;&gt;Twelfth_Night&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/R2.json&#39;&quot;&gt;King_Richard_II&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/R3.json&#39;&quot;&gt;King_Richard_III&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/2H4.json&#39;&quot;&gt;Henry_IV_ii&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/H5.json&#39;&quot;&gt;King_Henry_V&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/1H6.json&#39;&quot;&gt;Henry_VI_i&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/2H6.json&#39;&quot;&gt;Henry_VI_ii&lt;/option&gt;
        &lt;option value=&quot;&#39;https://s3.amazonaws.com/duhaime-shakespeare/folger-json/3H6.json&#39;&quot;&gt;Henry_VI_iii&lt;/option&gt;
    &lt;/select&gt;   

    Order:
    &lt;select class=&quot;play-menu form-group select-wrapper form-control&quot; id=&quot;order&quot;&gt;
        &lt;option value=&quot;name&quot;&gt;by Name&lt;/option&gt;
        &lt;option value=&quot;count&quot;&gt;by Frequency&lt;/option&gt;
        &lt;option value=&quot;group&quot;&gt;by Cluster&lt;/option&gt;
        &lt;option value=&quot;gender&quot;&gt;by Gender&lt;/option&gt;
    &lt;/select&gt;
    
    Color:
    &lt;select id=&quot;colorDropdown&quot;&gt;
        &lt;option value=&quot;gender&quot;&gt;by Gender&lt;/option&gt;
        &lt;option value=&quot;cluster&quot;&gt;by Cluster&lt;/option&gt;
    &lt;/select&gt;
&lt;/p&gt;
&lt;/aside&gt;

&lt;div id=&quot;cooccurrence&quot;&gt;&lt;/div&gt;
&lt;script async=&quot;&quot; src=&quot;js/cooccurrence.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In this visualization, “Frequency” represents the number of times a character appears on stage, “Gender” is indicated by the markup within the Folger Shakespeare Digital Collection XML (red = female, blue = male, green = unspecified), and “Cluster” reflects the subgroup of characters with whom a given character regularly appears, as determined by a fast greedy modularity ranking algorithm. Interacting with this plot allows one to uncover a number of insights. In the first place, we can see that the Histories consistently feature more “clusters” of characters than do Comedies or Tragedies. That is to say, while Comedies tend to be wildly interconnected affairs, Histories tend to include many small, isolated groups of characters that interact rather little with each other.  Looking at the gender dynamics of these groups, we can also see that in Comedies such as &lt;i&gt;Merry Wives of Windsor&lt;/i&gt; and Histories such as &lt;i&gt;Richard III&lt;/i&gt; and &lt;i&gt;Henry V&lt;/i&gt;, female characters tend to appear on stage together, almost creating a coherent collective over the course of the play.&lt;/p&gt;

&lt;p&gt;Finally, a number of female characters—such as Queen Margaret in &lt;i&gt;2 Henry VI&lt;/i&gt; and Adrianna in &lt;i&gt;Comedy of Errors&lt;/i&gt;—appear on stage more frequently than any other character in their respective plays, despite the fact that they say fewer words than their respective plays&#39; most outspoken characters. That is to say, their visual presence on stage is disproportionate to their verbal presence on stage. This raises a number of questions: To what extent were female characters meant to fulfill the role of a spectacle in Shakespearean drama? It’s difficult to imagine that the male players who acted as females projected authentic feminine voices. Did the limitations of imitative speech help mitigate the number of lines given to these prominent female characters? These and other questions remain to be explored in future work.&lt;/p&gt;

</description>
        <pubDate>Sun, 13 Dec 2015 10:24:24 -0500</pubDate>
        <link>http://yourdomain.com/posts/visualizing-shakespearean-characters</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/visualizing-shakespearean-characters</guid>
        
        
        <category>digital-humanities</category>
        
        <category>shakespeare</category>
        
      </item>
    
      <item>
        <title>Clustering Semantic Vectors with Python</title>
        <description>&lt;p&gt;Google’s Word2Vec and Stanford’s GloVe have recently offered two fantastic open source software packages capable of transposing words into a high dimension vector space. In both cases, a vector’s position within the high dimensional space gives a good indication of the word’s semantic class (among other things), and in both cases these vector positions can be used in a variety of applications. In the post below, I’ll discuss one approach you can take to clustering the vectors into coherent semantic groupings.&lt;/p&gt;

&lt;p&gt;Both Word2Vec and GloVe can create vector spaces given a large training corpus, but both maintain pretrained vectors as well. To get started with ~1GB of pretrained vectors from GloVe, one need only run the following lines:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;wget http://www-nlp.stanford.edu/data/glove.6B.300d.txt.gz
gunzip glove.6B.300d.txt.gz&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you unzip and then glance at glove.6B.300d.txt, you’ll see that it’s organized as follows:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;the 0.04656 0.21318 -0.0074364 [...] 0.053913
, -0.25539 -0.25723 0.13169 [...] 0.35499
. -0.12559 0.01363 0.10306 [...] 0.13684
of -0.076947 -0.021211 0.21271 [...] -0.046533
to -0.25756 -0.057132 -0.6719 [...] -0.070621
[...]
sandberger 0.429191 -0.296897 0.15011 [...] -0.0590532&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Each new line contains a token followed by 300 signed floats, and those values appear to be organized from most to least common. Given this ready format, it’s fairly straightforward to get straight to clustering!&lt;/p&gt;

&lt;p&gt;There are a variety of methods for clustering vectors, including density-based clustering, hierarchical clustering, and centroid clustering. One of the most intuitive and most commonly used centroid-based methods is K-Means. Given a collection of points in a space, K-Means uses a Hunger Games style random lottery to pick a few lucky points (colored green below), then assigns each of the non-lucky points to the lucky point to which it’s closest. Using these preliminary groupings, the next step is to find the “centroid” (or geometric center) of each group, using the same technique one would use to find the center of a square. These centroids become the new lucky points, and again each non-lucky point is again assigned to the lucky point to which it’s closest. This process continues until the centroids settle down and stop moving, after which the clustering is complete. Here’s a nice visual description of K-Means &lt;a href=&quot;http://shabal.in/visuals.html&quot;&gt;[source]&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post_images/clustering_semantic_vectors/kmeans.gif&quot; id=&quot;gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To cluster the GloVe vectors in a similar fashion, one can use the sklearn package in Python, along with a few other packages:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;from __future__ import division
from sklearn.cluster import KMeans 
from numbers import Number
from pandas import DataFrame
import sys, codecs, numpy&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It will also be helpful to build a class to mimic the behavior of autovivification in Perl, which is essentially the process of creating new default hash values given a new key. In Python, this behavior is available through collections.defaultdict(), but the latter isn’t serializable, so the following class is handy. Given an input key it hasn’t seen, the class will create an empty list as the corresponding hash value:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;class autovivify_list(dict):
    &amp;#39;&amp;#39;&amp;#39;A pickleable version of collections.defaultdict&amp;#39;&amp;#39;&amp;#39;
    def __missing__(self, key):
        &amp;#39;&amp;#39;&amp;#39;Given a missing key, set initial value to an empty list&amp;#39;&amp;#39;&amp;#39;
        value = self[key] = []
        return value

    def __add__(self, x):
        &amp;#39;&amp;#39;&amp;#39;Override addition for numeric types when self is empty&amp;#39;&amp;#39;&amp;#39;
        if not self and isinstance(x, Number):
            return x
        raise ValueError

    def __sub__(self, x):
        &amp;#39;&amp;#39;&amp;#39;Also provide subtraction method&amp;#39;&amp;#39;&amp;#39;
        if not self and isinstance(x, Number):
            return -1 * x
        raise ValueError&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We also want a method to read in a vector file (e.g. glove.6B.300d.txt) and store each word and the position of that word within the vector space. Because reading in and analyzing some of the larger GloVe files can take a long time, to get going quickly one can limit the number of lines to read from the input file by specifying a global value (n_words), which is defined later on:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;def build_word_vector_matrix(vector_file, n_words):
    &amp;#39;&amp;#39;&amp;#39;Return the vectors and labels for the first n_words in vector file&amp;#39;&amp;#39;&amp;#39;
    numpy_arrays = []
    labels_array = []
    with codecs.open(vector_file, &amp;#39;r&amp;#39;, &amp;#39;utf-8&amp;#39;) as f:
        for c, r in enumerate(f):
            sr = r.split()
            labels_array.append(sr[0])
            numpy_arrays.append( numpy.array([float(i) for i in sr[1:]]) )

            if c == n_words:
                return numpy.array( numpy_arrays ), labels_array

    return numpy.array( numpy_arrays ), labels_array&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Scikit-Learn’s implementation of K-Means returns an object (cluster_labels in these snippets) that indicates the cluster to which each input vector belongs. That object doesn’t tell one which word belongs in each cluster, however, so the following method takes care of this. Because all of the words being analyzed are stored in labels_array and the cluster to which each word belongs is stored in cluster_labels, the following method can easily map those two sequences together:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;def find_word_clusters(labels_array, cluster_labels):
    &amp;#39;&amp;#39;&amp;#39;Return the set of words in each cluster&amp;#39;&amp;#39;&amp;#39;
    cluster_to_words = autovivify_list()
    for c, i in enumerate(cluster_labels):
        cluster_to_words[ i ].append( labels_array[c] )
    return cluster_to_words&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Finally, we can call the methods above, perform K-Means clustering, and print the contents of each cluster with the following block:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;if __name__ == &amp;quot;__main__&amp;quot;:
    input_vector_file = sys.argv[1] # Vector file input (e.g. glove.6B.300d.txt)
    n_words           = int(sys.argv[2]) # Number of words to analyze 
    reduction_factor  = float(sys.argv[3]) # Amount of dimension reduction {0,1}
    clusters_to_make  = int( n_words * reduction_factor ) # Number of clusters to make
    df, labels_array  = build_word_vector_matrix(input_vector_file, n_words)
    kmeans_model      = KMeans(init=&amp;#39;k-means++&amp;#39;, n_clusters=clusters_to_make, n_init=10)
    kmeans_model.fit(df)

    cluster_labels    = kmeans_model.labels_
    cluster_inertia   = kmeans_model.inertia_
    cluster_to_words  = find_word_clusters(labels_array, cluster_labels)

    for c in cluster_to_words:
        print cluster_to_words[c]
        print &amp;quot;\n&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The full script is available &lt;a href=&quot;https://gist.github.com/duhaime/9330473f9a4e288f00af&quot;&gt;here&lt;/a&gt;. To run it, one needs to specify the vector file to be read in, the number of words one wishes to sample from that file (one can of course read them all, but doing so can take some time), and the “reduction factor”, which determines the number of clusters to be made. If one specifies a reduction factor of .1, for instance, the routine will produce n*.1 clusters, where n is the number of words sampled from the file. The following command reads in the first 10,000 words, and produces 1,000 clusters:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;python cluster_vectors.py glove.6B.300d.txt 10000 .1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The output of this command is the series of clusters produced by the K-Means clustering:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;[u&amp;#39;Chicago&amp;#39;, u&amp;#39;Boston&amp;#39;, u&amp;#39;Houston&amp;#39;, u&amp;#39;Atlanta&amp;#39;, u&amp;#39;Dallas&amp;#39;, u&amp;#39;Denver&amp;#39;, u&amp;#39;Philadelphia&amp;#39;, u&amp;#39;Baltimore&amp;#39;, u&amp;#39;Cleveland&amp;#39;, u&amp;#39;Pittsburgh&amp;#39;, u&amp;#39;Buffalo&amp;#39;, u&amp;#39;Cincinnati&amp;#39;, u&amp;#39;Louisville&amp;#39;, u&amp;#39;Milwaukee&amp;#39;, u&amp;#39;Memphis&amp;#39;, u&amp;#39;Indianapolis&amp;#39;, u&amp;#39;Auburn&amp;#39;, u&amp;#39;Dame&amp;#39;]

[u&amp;#39;Product&amp;#39;, u&amp;#39;Products&amp;#39;, u&amp;#39;Shipping&amp;#39;, u&amp;#39;Brand&amp;#39;, u&amp;#39;Customer&amp;#39;, u&amp;#39;Items&amp;#39;, u&amp;#39;Retail&amp;#39;, u&amp;#39;Manufacturer&amp;#39;, u&amp;#39;Supply&amp;#39;, u&amp;#39;Cart&amp;#39;, u&amp;#39;SKU&amp;#39;, u&amp;#39;Hardware&amp;#39;, u&amp;#39;OEM&amp;#39;, u&amp;#39;Warranty&amp;#39;, u&amp;#39;Brands&amp;#39;]

[u&amp;#39;home&amp;#39;, u&amp;#39;house&amp;#39;, u&amp;#39;homes&amp;#39;, u&amp;#39;houses&amp;#39;, u&amp;#39;housing&amp;#39;, u&amp;#39;offices&amp;#39;, u&amp;#39;household&amp;#39;, u&amp;#39;acres&amp;#39;, u&amp;#39;residence&amp;#39;]

[...]

[u&amp;#39;Night&amp;#39;, u&amp;#39;Disney&amp;#39;, u&amp;#39;Magic&amp;#39;, u&amp;#39;Dream&amp;#39;, u&amp;#39;Ultimate&amp;#39;, u&amp;#39;Fantasy&amp;#39;, u&amp;#39;Theme&amp;#39;, u&amp;#39;Adventure&amp;#39;, u&amp;#39;Cruise&amp;#39;, u&amp;#39;Potter&amp;#39;, u&amp;#39;Angels&amp;#39;, u&amp;#39;Adventures&amp;#39;, u&amp;#39;Dreams&amp;#39;, u&amp;#39;Wonder&amp;#39;, u&amp;#39;Romance&amp;#39;, u&amp;#39;Mystery&amp;#39;, u&amp;#39;Quest&amp;#39;, u&amp;#39;Sonic&amp;#39;, u&amp;#39;Nights&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I’m currently using these word clusters for fuzzy plagiarism detection, but they can serve a wide variety of purposes. If you find them helpful for a project you’re working on, feel free to drop me a note below!&lt;/p&gt;

</description>
        <pubDate>Sat, 12 Sep 2015 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/clustering-semantic-vectors</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/clustering-semantic-vectors</guid>
        
        
        <category>word-embeddings</category>
        
        <category>clustering</category>
        
      </item>
    
      <item>
        <title>Crosslingual Plagiarism Detection with Scikit-Learn</title>
        <description>&lt;p&gt;Oliver Goldsmith, one of the great poets, playwrights, and historians of science from the Enlightenment, was many things. He was “an idle, orchard-robbing schoolboy; a tuneful but intractable sizar of Trinity; a lounging, loitering, fair-haunting, flute-playing Irish ‘buckeen.’” He was also a brilliant plagiarist. Goldsmith frequently borrowed whole sentences and paragraphs from French &lt;i&gt;philosophes&lt;/i&gt; such as Voltaire and Diderot, closely translating their works into his own voluminous books without offering so much as a word that the passages were taken from elsewhere. Over the last several months, I have worked with several others to study the ways Goldsmith adapted and freely translated these source texts into his own writing in order to develop methods that can be used to discover crosslingual text reuse. By outlining below some of the methods that I have found useful within this field of research, the following post attempts to show how automated methods can be used to further advance our understanding of the history of authorship.&lt;/p&gt;

&lt;h4&gt;Sample Training Data&lt;/h4&gt;

&lt;p&gt;In order to identify the passages within Goldsmith’s corpus that were taken from other writers, I decided to train a machine learning algorithm to differentiate between plagiarisms and non-plagiarisms. To distinguish between these classes of writing, John Dillon and I collected a large number of plagiarized and non-plagiarized passages within Goldsmith’s writing, and provided annotations to identify whether the target passage had been plagiarized or not. Here are a few sample rows from the training data:&lt;/p&gt;

&lt;div id=&quot;goldsmith-training-table&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;&lt;th&gt;French Source&lt;/th&gt;&lt;th&gt;Goldsmith Text&lt;/th&gt;&lt;th&gt;Plagiarism&lt;/th&gt;&lt;/tr&gt;

    &lt;tr&gt;&lt;td&gt;Bothwell eut toute l&#39;insolence qui suit les grands crimes. Il assembla les principaux seigneurs, et leur fit signer un écrit, par lequel il était dit expressément que la reine ne se pouvait dispenser de l&#39;éspouser, puisqu&#39;il l&#39;avait enlevée, et qu&#39;il avait couché avec elle.&lt;/td&gt;&lt;td&gt;Bothwell was possessed of all the insolence which attends great crimes: he assembled the principal Lords of the state, and compelled them to sign an instrument, purporting, that they judged it the Queen&#39;s interest to marry Bothwell, as he had lain with her against her will.&lt;/td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;
  
    &lt;tr&gt;&lt;td&gt;Histoire c&#39;est le récit des faits donnés pour vrais; au contraire de la fable, qui est le récit des faits donnés pour faux.&lt;/td&gt;&lt;td&gt;In the early part of history a want of real facts hath induced many to spin out the little that was known with conjecture.&lt;/td&gt;&lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;

    &lt;tr&gt;&lt;td&gt;La meilleure maniere de connoître l&#39;usage qu&#39;on doit faire de l&#39; esprit, est de lire le petit nombre de bons ouvravrages de génie qu&#39;on a dans les langues savantes &amp;amp; dans la nôtre.&lt;/td&gt;&lt;td&gt;The best method of knowing the true use to be made of wit is, by reading the small number of good works, both in the learned languages, and in our own.&lt;/td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;  
    &lt;tr&gt;&lt;td&gt;Comme il y a en Peinture différentes écoles, il y en a aussi en Sculpture, en Architecture, en Musique, &amp;amp; en général dans tous les beaux Arts.&lt;/td&gt;&lt;td&gt;A school in the polite arts, properly signifies, that succession of artists which has learned the principles of the art from some eminent master, either by hearing his lessons, or studying his works.&lt;/td&gt;&lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;&lt;/tr&gt;  
  
    &lt;tr&gt;&lt;td&gt;Des étoiles qui tombent, des montagnes qui se fendent, des fleuves qui reculent, le Soleil &amp;amp; la Lune qui se dissolvent, des comparaisons fausses &amp;amp; gigantesques, la nature toûjours outrée, sont le caractere de ces écrivains, parce que dans ces pays où l&#39;on n&#39;a jamais parlé en public.&lt;/td&gt;&lt;td&gt;Falling stars, splitting mountains, rivers flowing to their sources, the sun and moon dissolving, false and unnatural comparisons, and nature everywhere exaggerated, form the character of these writers; and this arises from their never, in these countries, being permitted to speak in public.&lt;/td&gt;&lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;&lt;/tr&gt;

  &lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Given this training data, the goal was to identify some features that commonly appear in Goldsmith’s plagiarized passages but don’t commonly appear in his non-plagiarized passages. If we could derive a set of features that differentiate between these two classes, we would be ready to search through Goldsmith’s corpus and tease out only those passages that had been borrowed from elsewhere.&lt;/p&gt;

&lt;h4&gt;FEATURE SELECTION: ALZAHRANI SIMILARITY&lt;/h4&gt;

&lt;p&gt;Because a plagiarized passage can be expected to have language that is similar but not necessarily identical to the language used within the plagiarized source text, I decided to test some fuzzy string similarity measures. One of the more promising leads on this front was adapted from the work of Salha M. Alzahrani et al. [2012], who has produced a number of great papers on plagiarism detection. The specific similarity measure adapted from Alzahrani calculates the similarity between two passages (call them Passage A and Passage B) in the following way:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;def alzahrani_similarity( a_passage, b_passage ):

    # Create a similarity counter and set its value to zero
    similarity = 0

    # For each word in Passage A
    for a_word in a_passage:

        # If that word is in Passage B
        if a_word in b_passage:

            # Add one to the similarity counter
            similarity += 1

        # Otherwise,
        else:

            # For each word in Passage B
            for b_word in b_passage:    

                # If the current word from Passage A is a synonym of the current word from Passage B,
                if a_word in find_synonyms( b_word ):

                    # Add one half to the similarity counter
                    similarity += .5
                    break

    # Finally, divide the similarity score by the number of words in the longer passage
    return similarity / max( len(a_passage), len(b_passage) )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To prepare the data for this algorithm, I used the &lt;a href=&quot;http://pythonhosted.org/goslate/&quot;&gt;Google Translate API&lt;/a&gt; to translate French texts into English, the &lt;a href=&quot;https://words.bighugelabs.com/api.php&quot;&gt;Big Huge Labs Thesaurus API&lt;/a&gt; to collect synonyms for each word in Passage B, and the &lt;a href=&quot;http://www.nltk.org/&quot;&gt;NLTK&lt;/a&gt; to clean the resulting texts (dropping stop words, removing punctuation, etc.). Once these resources were prepared, I used an implementation of the algorithm described above to calculate the “similarity” between the paired passages in the training data. As one can see, the similarity value returned by this algorithm discriminates reasonably well between plagiarized and non-plagiarized passages:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/crosslingual_plagiarism_detection/alzahrani_aggregate.png&quot; alt=&quot;Alzahrani Similarity&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The y-axis here is discrete–each data point represents either a plagiarized pair of passages (such as those in the training data discussed above), or a non-plagiarized pair of passages. The x-axis is really the important axis. The further to the right a point falls on this axis, the greater the length-normalized similarity score for the passage pair. As one would expect, plagiarized passages have much higher similarity scores than non-plagiarized passages.&lt;/p&gt;

&lt;p&gt;In order to investigate how sensitive this similarity method is to passage length, I iterated over all sub-windows of &lt;i&gt;n&lt;/i&gt; words within the training data, and used the same similarity method to calculate the similarity of the sub-window within the text. When &lt;i&gt;n&lt;/i&gt; is five, for instance, one would compare the first five words of Passage A to the first five from Passage B. After storing that value, one would compare words two through six from Passage A to words one through five of Passage B, then words three through seven from Passage A to words one through five of Passage B, proceeding in this way until all five-word windows had been compared. Once all of these five-word scores are calculated, only the maximum score is retained, and the rest are discarded. The following plot shows that as the number of words in the sub-window increases, the separation between plagiarized and non-plagiarized passages also increases:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/crosslingual_plagiarism_detection/alzahrani_subwindows.png&quot; alt=&quot;Alzahrani Similarity Subwindows&quot; style=&quot;width:100%; height:100%&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;FEATURE SELECTION: WORD2VEC SIMILARITY&lt;/h4&gt;
&lt;p&gt;Although the method discussed above provides helpful separation between plagiarized and non-plagiarized passages, it reduces word pairs to one of three states: equivalent, synonymous, and irrelevant. Intuitively, this model feels limited, because one senses that words can have &lt;i&gt;degrees&lt;/i&gt; of similarity. Consider the words &lt;i&gt;small&lt;/i&gt;, &lt;i&gt;tiny&lt;/i&gt;, and &lt;i&gt;humble&lt;/i&gt;. The thesaurus discussed above identifies these terms as synonyms, and the algorithm described above essentially treats the words as interchangeable synonyms. This is slightly unsatisfying because the word &lt;i&gt;small&lt;/i&gt; seems more similar to the word &lt;i&gt;tiny&lt;/i&gt; than the word &lt;i&gt;humble&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;To capture some of these finer gradations in meaning, I called on &lt;a href=&quot;https://code.google.com/p/word2vec/&quot;&gt;Word2Vec&lt;/a&gt;, a method that uses backpropagation to represent words in high-dimensional vector spaces. Once a word has been transposed into this vector space, one can compare a word’s vector to another word’s vector and obtain a measure of the similarity of those words. The following snippet, for instance, uses a cosine distance metric to measure the degree to which tiny and humble are similar to the word small:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;from gensim.models.word2vec import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity        

# Load the Google pretrained word vectors
model = Word2Vec.load_word2vec_format(&amp;#39;../google_pretrained_word_vectors/GoogleNews-vectors-negative300.bin.gz&amp;#39;, binary=True)

# Obtain the vector representations of three words
v1 = model[ &amp;quot;small&amp;quot; ]
v2 = model[ &amp;quot;tiny&amp;quot; ]
v3 = model[ &amp;quot;humble&amp;quot; ]

# Measure the similarity of &amp;quot;tiny&amp;quot; and &amp;quot;humble&amp;quot; to the word &amp;quot;small&amp;quot;
for v in [v2,v3]:
    print cosine_similarity(v1, v)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Running this script returns [[ 0.71879274]] and [[ 0.29307675]] respectively, which is to say Word2Vec can recognize that the word &lt;i&gt;small&lt;/i&gt; is more similar to &lt;i&gt;tiny&lt;/i&gt; than it is to &lt;i&gt;humble&lt;/i&gt;. Because Word2Vec allows one to calculate these fine gradations of word similarity, it does a great job calculating the similarity of passages from the Goldsmith training data. The following plot shows the separation achieved by running a modified version of the “Alzahrani algorithm” described above, using this time Word2Vec to measure word similarity:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/crosslingual_plagiarism_detection/word2vec_aggregate.png&quot; alt=&quot;Word2Vec Similarity&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As one can see, the Word2Vec similarity measure achieves very promising separation between plagiarized and non-plagiarized passage pairs. By repeating the subwindow method described above, one can identify the critical value wherein separation between plagiarized and non-plagiarized passages is best achieved with a Word2Vec similarity metric:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/crosslingual_plagiarism_detection/word2vec_subwindows.png&quot; alt=&quot;Word2Vec Subwindow Similarity&quot; style=&quot;width:100%; height:100%&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;FEATURE SELECTION: SYNTACTIC SIMILARITY&lt;/h4&gt;

&lt;p&gt;Much like the semantic features discussed above, syntactic similarity can also serve as a clue of plagiarism. While a thoroughgoing pursuit of syntactic features might lead one deep into sophisticated analysis of dependency trees, it turns out one can get reasonable results by simply examining the distribution of part of speech tags within Goldsmith’s plagiarisms and their source texts. Using the Stanford Part of Speech (POS) Tagger’s French and English models, and a custom mapping I put together to link the French POS tags to the universal tagset, I transformed each of the paired passages in the training data into a POS sequence such as the following:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;[(u&amp;#39;Newton&amp;#39;, u&amp;#39;NNP&amp;#39;), (u&amp;#39;appeared&amp;#39;, u&amp;#39;VBD&amp;#39;),...,(u&amp;#39;amazing&amp;#39;, u&amp;#39;JJ&amp;#39;), (u&amp;#39;.&amp;#39;, u&amp;#39;.&amp;#39;)]
[(u&amp;#39;Newton&amp;#39;, u&amp;#39;NPP&amp;#39;), (u&amp;#39;parut&amp;#39;, u&amp;#39;V&amp;#39;),...,(u&amp;#39;nouvelle:&amp;#39;, u&amp;#39;CL&amp;#39;),(u&amp;#39;.&amp;#39;, u&amp;#39;.&amp;#39;)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using these sequences, two similarity metrics were used to measure the similarity between each of the paired passages in the training data. The first measure (on the x-axis below) simply measured the cosine distance between the two POS sequences; the second measure (on the y-axis below) calculated the longest common POS substring between the two passages. As one would expect, plagiarized passages tend to have higher values in both categories:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/crosslingual_plagiarism_detection/syntax.png&quot; alt=&quot;Syntactic Similarity&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;CLASSIFIER RESULTS&lt;/h4&gt;
&lt;p&gt;From the similarity metrics discussed above, I selected a bare-bones set of six features that could be fed to a plagiarism classifier: (1) the aggregate “Alzahrani similarity” score, (2) the maximum six-gram Alzahrani similarity score, (3) the aggregate Word2Vec similarity score, (4) the cosine distance between the part of speech tag sets, (5) the longest common part of speech string, and (6) the longest contiguous common part of speech string. Those values were all represented in a matrix format with one pair of passages per row and one feature per column. Once this matrix was prepared, a small selection of classifiers hosted within Python’s &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;Scikit Learn&lt;/a&gt; library were chosen for comparison. Cross-classifier comparison is valuable, because different classifiers use very different logic to classify observations. The following plot from the Scikit Learn documentation shows that using a common set of input data (the first column below), the various classifiers in the given row classify that data rather differently:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/post_images/crosslingual_plagiarism_detection/classifier_comparison.png&quot; alt=&quot;Classifier Comparison&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to avoid prejudging the best classifier for the current task, half a dozen classifiers were selected and evaluated with hold one out tests. That is to say, for each observation in the training data, all other rows were used to train the given classifier, and the trained classifier was asked to predict whether the left-out observation was a plagiarism or not. Because this is a two class prediction task (each observation either is or is not an instance of plagiarism), the baseline success rate is 50%. Any performance below this baseline would be worse than random guessing. Happily, all of the classifiers achieved success rates that greatly exceeded this baseline value:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/crosslingual_plagiarism_detection/classification_results.png&quot; alt=&quot;Classification Results&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Generally speaking, precision values were higher than recall, perhaps because some of the plagiarisms in the training data were fuzzier than others. Nevertheless, these accuracy values were high enough to warrant further exploration of Goldsmith’s writing. Using the array of features discussed above and others to be discussed in a subsequent post, I tracked down a significant number of plagiarisms that were not part of the training data, including the following outright translations from the Encyclopédie:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;French Source&lt;/th&gt;&lt;th&gt;Goldsmith Text&lt;/th&gt;&lt;/tr&gt;

  &lt;tr&gt;&lt;td&gt;Il n&#39;est point douteux que l&#39; Empire , composé d&#39;un grand nombre de membres très-puissans, ne dût être regardé comme un état très-respectable à toute l&#39;Europe, si tous ceux qui le composent concouroient au bien général de leur pays. Mais cet état est sujet à de très-grands inconvéniens: l&#39;autorité du chef n&#39;est point assez grande pour se faire écouter: la crainte, la défiance, la jalousie, regnent continuellement entre les membres: personne ne veut céder en rien à son voisin: les affaires les plus sérieuses les plus importantes pour tout le corps sont quelquefois négligées pour des disputes particulieres, de préséance, d&#39;étiquette, de droits imaginaires d&#39;autres minuties.&lt;/td&gt;&lt;td&gt;It is not to be doubted but that the empire, composed as it is of several very powerful states, must be considered as a combination that deserves great respect from the other powers of Europe, provided that all the members which compose it would concur in the common good of their country. But the state is subject to very great inconveniences; the authority of the head is not great enough to command obedience; fear, distrust, and jealousy reign continually among the members; none are willing to yield in the least to their neighbours; the most serious and the most important affairs with respect to the community, are often neglected for private disputes, for precedencies, and all the imaginary privileges of misplaced ambition.&lt;/td&gt;&lt;/tr&gt;  
  
  &lt;tr&gt;&lt;td&gt;L&#39; Eloquence , dit M. de Voltaire, est née avant les regles de la Rhétorique, comme les langues se sont formées avant la Grammaire.&lt;/td&gt;&lt;td&gt;Thus we see, eloquence is born with us before the rules of rhetoric, as languages have been formed before the rules of grammar.&lt;/td&gt;&lt;/tr&gt;
  
  &lt;tr&gt;&lt;td&gt;L&#39; empire Germanique, dans l&#39;état où il est aujourd&#39;hui, n&#39;est qu&#39;une portion des états qui étoient soûmis à Charlemagne. Ce prince possédoit la France par droit de succession; il avoit conquis par la force des armes tous les pays situés depuis le Danube jusqu&#39;à la mer Baltique; il y réunit le royaume de Lombardie, la ville de Rome son territoire, ainsi que l&#39;exarchat de Ravennes, qui étoient presque les seuls domaines qui restassent en Occident aux empereurs de Constantinople.&lt;/td&gt;&lt;td&gt;The empire of Germany, in its present state is only a part of those states that were once under the dominion of Charlemagne. This prince was possessed of France by right of succession: he had conquered by force of arms all the countries situated between the Baltic Sea and the Danube. He added to his empire the kingdom of Lombardy, the city of Rome and its territory, together with the exarchate of Ravenna, which were almost the only possessions that remained in the West to the emperors of Constantinople.&lt;/td&gt;&lt;/tr&gt;

  &lt;tr&gt;&lt;td&gt;Il n&#39;est point de genre de poésie qui n&#39;ait son caractere particulier; cette diversité, que les anciens  observerent si religieusement, est fondée sur la nature même des sujets imités par les poëtes. Plus leurs imitations sont vraies, mieux ils ont rendu les caracteres qu&#39;ils avoient à exprimer....Ainsi l&#39;églogue ne quitte pas ses chalumeaux pour entonner la trompette, l&#39; élégie n&#39;emprunte point les sublimes accords de la lyre.&lt;/td&gt;&lt;td&gt;There is no species of poetry that has not its particular character; and this diversity, which the ancients have so religiously observed, is founded in nature itself. The more just their imitations are found, the more perfectly are those characters distinguished. Thus the pastoral never quits his pipe, in order to sound the trumpet; nor does elegy venture to strike the lyre.&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h4&gt;CONCLUSION&lt;/h4&gt;
&lt;p&gt;Samuel Johnson once observed that Oliver Goldsmith was “at no pains to fill his mind with knowledge. He transplanted it from one place to another; and it did not settle in his mind; so he could not tell what was his in his own books” (&lt;i&gt;Life of Johnson&lt;/i&gt;). Reading the borrowed passages above, one can perhaps understand why Goldsmith struggled to recall what he had written in his books–much of his writing was not really his. As scholars continue to advance the art of detecting textual reuse, we will be better equipped to map these borrowed words at larger and more ambitious scales. For the present, writers like Goldsmith offer plenty of data on which to hone those methods.&lt;/p&gt;

&lt;div style=&quot;text-align: center&quot;&gt;* * *&lt;/div&gt;
&lt;p&gt;This work has benefitted enormously from conversations with a number of others. Antonis Anastasopoulos, David Chiang, Michael Clark, John Dillon, and Kenton Murray of Notre Dame’s Text Analysis Group, and Thom Bartold, Dan Hepp, and Jens Wessling of ProQuest offered key analytic insights, and Mark Olsen and Glenn Roe of the University of Chicago’s ARTFL group shared essential data. I am grateful for the generous help each of you has provided. Code is available &lt;a href=&quot;https://github.com/duhaime/detect_reuse&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 19 Jul 2015 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/crosslingual-plagiarism-detection</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/crosslingual-plagiarism-detection</guid>
        
        
        <category>classification</category>
        
        <category>digital-humanities</category>
        
        <category>word-embeddings</category>
        
        <category>text-reuse</category>
        
      </item>
    
      <item>
        <title>Mapping the Early English Book Trade</title>
        <description>&lt;p&gt;Historians often call attention to the tremendous influence the 1710 Act of Anne had on the early English book trade. Commonly identified as the origin of modern copyright law, the Act laid the statutory foundations for fixed-term copyright in England, extended the ability to hold such copyrights to all individuals, and eventually toppled the monopoly that London booksellers had held on English printing since the incorporation of the Stationers’ Company in 1557. Reading scholarship on this legal development over the last few months, I became curious to see how well the English Short Title Catalogue (&lt;a href=&quot;http://estc.bl.uk/F/?func=file&amp;amp;file_name=login-bl-estc&quot;&gt;ESTC&lt;/a&gt;) could substantiate some of the claims made in discussions of the Act. The ESTC seemed an ideal resource for this kind of analysis because, as Stephen Tabor has written, it represents “the fullest and most up-to-date bibliographical account of ‘English’ printing (in the broadest sense) for its first 328 years” (&lt;a href=&quot;http://muse.jhu.edu/journals/lbt/summary/v008/8.4tabor.html&quot;&gt;367&lt;/a&gt;). The database lists the authors, titles, imprint lines, publication dates, and many other metadata fields for each of the ~470,000 editions known to have been printed in England or its colonies between 1473 and 1800, and can therefore serve as a helpful resource with which to investigate the relationship between copyright law and literary history in the early modern period.&lt;/p&gt;

&lt;p&gt;One of the debates surrounding the Act of Anne concerns the degree to which the statute altered the geography of the English book trade. Prior to the passage of the Act, legal historian Diane Zimmerman notes, the Stationers’ Company dominated the book industry, and because the company’s printers were primarily stationed in London, the book trade was also centered in the metropole. With the passage of the Statute of Anne, however, authors could sell or trade their copyrights to printers outside of London: “Now any printer [or] bookseller, wherever located within the country, could register a copyright with the Company” and “since purchasers of the copies could be located anywhere in the United Kingdom, the Stationers’ Company did not regain its monopoly [on the book trade]” (7). Contra Zimmerman, William Patry argues that the Act of Anne failed to undermine London’s control of the book trade: “After the Statute of Anne, as before,” he writes, “the only purchasers of authors’ works were a small group of London booksellers” (&lt;a href=&quot;https://books.google.com/books?id=8-4catWPy84C&amp;amp;q=%22small+group+of+London+booksellers%22#v=snippet&amp;amp;q=%22small%20group%20of%20London%20booksellers%22&amp;amp;f=false&quot;&gt;84&lt;/a&gt;). To investigate what the ESTC had to say on this question, I compared the geographical distribution of English printers in the half centuries before and after the passage of the Act (click for full size):&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;img-link-wrapper&quot; href=&quot;/images/post_images/mapping_early_english_books/provincial_printing.png&quot; data-lightbox=&quot;provincial_printing&quot; data-title=&quot;&quot;&gt;
  &lt;img class=&quot;img-link&quot; src=&quot;/images/post_images/mapping_early_english_books/provincial_printing.png&quot; alt=&quot;Provincial Printing&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The usual cautions concerning false imprints and varying survival rates notwithstanding, the ESTC clearly demonstrates the decentralization of English printing in the wake of the Act of Anne. London of course remained the primary site of publication throughout the years covered by the ESTC—publishing two-thirds of all records from the period—though its annual share in the trade fell quite dramatically across the eighteenth century:&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;center-image img-link-wrapper&quot; href=&quot;/images/post_images/mapping_early_english_books/london_printing.png&quot; data-lightbox=&quot;london_printing&quot; data-title=&quot;&quot;&gt;
  &lt;img class=&quot;center-image img-link&quot; src=&quot;/images/post_images/mapping_early_english_books/london_printing.png&quot; alt=&quot;London Printing&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One can explain some of that decline by examining the growth of printing in major metropolitan areas outside of London, such as Edinburgh (responsible for 6.5% of total editions in the ESTC), Dublin (5.4%), and Boston (3.7%), which claimed the second, third, and fourth overall largest shares of the book trade according to the ESTC:&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;center-image img-link-wrapper&quot; href=&quot;/images/post_images/mapping_early_english_books/publishing_beyond_london.png&quot; data-lightbox=&quot;publishing_beyond_london&quot; data-title=&quot;&quot;&gt;
  &lt;img class=&quot;center-image img-link&quot; src=&quot;/images/post_images/mapping_early_english_books/publishing_beyond_london.png&quot; alt=&quot;Publishing Beyond London&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Among these figures, the explosion of printing in Edinburgh after 1750 is particularly interesting, and appears to be the result of further changes in the legal code. As John Feather notes, “The Copyright Act of 1710 (8 Anne c. 21) implied, but did not state, that it was illegal to import any English-language books into England and Wales if they had been previously printed there” (&lt;a href=&quot;/assets/pdf/posts/mapping_early_books/feather_english_book_trade.pdf&quot;&gt;58&lt;/a&gt;). However, he continues, “the legislation in relation to Scotland seems to have lapsed in 1754-1755,” after which one observes tremendous growth in Scottish printing. Between 1750 and 1755, the five year average of Edinburgh printing as a percent of all printing recorded in the ESTC is 7.5%. This figure only continues to grow after the lapse of Scottish printing regulations noted by Feather: From 1755-1760, Edinburgh printing climbs to 9.0% of all printing for the five year period, from 1760-1765, the figure rises to 12.3%, and from 1765-1770, it reaches 14.4% of the ESTC totals for the five year range. These values are significant, because they suggest the real surge in the Scottish reprinting industry did not take place in the aftermath of the Donaldson v. Becket decision, as is commonly supposed, but rather with the lapse of Scottish reprinting regulations in 1755.&lt;/p&gt;

&lt;p&gt;Having plotted the changing geography of early English printing, I was curious to see whether the ESTC could shed new light on the debate concerning anonymous printing in the early modern period. Researchers like Jody Greene have argued that the Statute of Anne was in fact designed to help combat anonymous publishing insofar as it required authors to attach their names to works if they wished to obtain copyright protection for those works (&lt;a href=&quot;https://books.google.com/books?id=PFQchrtgYwcC&amp;amp;q=To+claim+responsibility+for+a+work+after+1710&amp;amp;hl=en#v=onepage&amp;amp;q=%22To%20claim%20responsibility%22&amp;amp;f=false&quot;&gt;4&lt;/a&gt;). Years ago, Michel Foucault pioneered a version of this thesis in his essay “&lt;a href=&quot;/pdf/posts/mapping_early_books/focault_what_is_an_author.pdf&quot;&gt;What is an Author?&lt;/a&gt;”, where he argued that the Act of Anne and its elaboration in eighteenth-century case law spurred the transition from a literary culture founded on anonymity to one founded on named authorship. More recently, however, Robert Griffin disputed such claims, arguing that “the historical record shows . . . there is no necessary relation between copyright and the appearance of the name of the author on the title page” (&lt;a href=&quot;https://muse.jhu.edu/journals/new_literary_history/v030/30.4griffin.html&quot;&gt;879&lt;/a&gt;). To map the changing rate of anonymity over time, I aggregated the number of anonymous and pseudonymous publications as percents of annual totals within the ESTC:&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;center-image img-link-wrapper&quot; href=&quot;/images/post_images/mapping_early_english_books/anonymous_publishing.png&quot; data-lightbox=&quot;anonymous_publishing&quot; data-title=&quot;&quot;&gt;
  &lt;img class=&quot;center-image img-link&quot; src=&quot;/images/post_images/mapping_early_english_books/anonymous_publishing.png&quot; alt=&quot;Anonymous Publishing&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The resulting plot shows great fluctuation in anonymous publications within the fifteenth and early sixteenth centuries, largely because of the tremendously small number of publications for those years. In 1492, for instance, the ESTC lists only 14 publications, all but two of which (S111337 and S120825) had identified authors, which results in an aggregate estimate of anonymity for the year of .142, or 14.2 percent. Despite the year to year fluctuations within early records, however, examining anonymity rates in the aggregate leads to legible patterns: one finds a marked decline in anonymous publication rates over the fifteenth and sixteenth centuries, a fairly steady rise across the seventeenth century, and a slow aggregate decline in the wake of the Act of Anne. This data supports some of the the findings of Joad Raymond—who examined a small sample of records from the period and found that “anonymity . . . became increasingly frequent” over the course of the seventeenth century (&lt;a href=&quot;https://books.google.com/books?id=DyMjW21HwHwC&amp;amp;q=168#v=onepage&amp;amp;q=%22Other%20changing%20tendencies%22&amp;amp;f=false&quot;&gt;168&lt;/a&gt;)—while challenging the popular thesis that anonymity thrived with the lapse of the Licensing Act in 1695.&lt;/p&gt;

&lt;p&gt;To plot the history of anonymity, though, is to beg a fundamental question: What exactly counts as an anonymous work? While the plot above treats works as anonymous only if their title pages are attributed to pseudonymous figures like “Isaac Bickerstaff” or to no author at all, there are other cases that one might well wish to classify as anonymous works. Consider the range of works attributed to “corporate” authors like the Royal Society of London or the English Parliament. Are works published by these entities anonymous publications? The way one answers this question will of course greatly affect the way one reads the history of anonymity. As a case in point, we could consult the following plot, which shows monarchical and parliamentary publishing during the seventeenth and eighteenth centuries:&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;center-image img-link-wrapper&quot; href=&quot;/images/post_images/mapping_early_english_books/parliamentarian_printing.png&quot; data-lightbox=&quot;parliamentarian_printing&quot; data-title=&quot;&quot;&gt;
  &lt;img class=&quot;center-image img-link&quot; src=&quot;/images/post_images/mapping_early_english_books/parliamentarian_printing.png&quot; alt=&quot;Parliamentarian Printing&quot; style=&quot;max-width:100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The points here represent yearly values, while the regression lines map the smoothed trends over time. For example, the release of the ESTC to which I had access indicates that James I and Charles I published a combined total of 82 works in 1625 (both served as monarch during the year), the English and Scottish Parliaments published a combined total of 4 works during the year, and the year’s total number of publications was 695, which means that monarchical publications account for 11.79 percent of the annual total while parliamentary publications account for only .5 percent  of the same. As one can see, treating the high volume of parliamentary publications from the period as “anonymous works” would create a serious spike in anonymity rates during the English Civil Wars, and would steadily inflate anonymity rates across the eighteenth century. On the other hand, refusing to include works of corporate authorship among anonymous publications (as I have done in the plot of anonymity above) makes it more difficult to answer the question: What exactly counts as anonymity in the early modern world? Whether one includes or excludes corporate authorship from the domain of anonymity, this plot of parliamentary and monarchical publications intrigues me because it maps so neatly onto the political history of the English Civil Wars: monarchical publications trump parliamentary output until the critical years of the early 1640’s, after which the Parliament assumes a predominance it holds throughout the Interregnum and only loses in the Restoration. Thereafter the monarchical voice triumphs until the Statute of Anne, after which point it rapidly loses ground. Examining this plot, I can’t help but wonder: To what extent is monarchical publishing a function of the crown’s political power, and to what extent is that political power a function of the monarch’s proximity to print?&lt;/p&gt;

&lt;div class=&quot;center-text&quot;&gt;* * *&lt;/div&gt;

&lt;p&gt;I want to thank Benjamin Pauley, Brian Geiger, and Virginia Schilling—each of whom kindly helped me to acquire the ESTC data on which the analysis above was performed—as well as Elliott Visconsi, whose intriguing questions on copyright history continue to motivate my ongoing research.&lt;/p&gt;

</description>
        <pubDate>Wed, 07 Jan 2015 10:24:24 -0500</pubDate>
        <link>http://yourdomain.com/posts/mapping-the-early-english-book-trade</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/mapping-the-early-english-book-trade</guid>
        
        
        <category>digital-humanities</category>
        
        <category>gis</category>
        
      </item>
    
      <item>
        <title>Classifing Shakeseparean Drama with Sparse Feature Sets</title>
        <description>&lt;p&gt;In her fantastic series of lectures on early modern England, Emma Smith identifies an interesting feature that differentiates the tragedies and comedies of Elizabethan drama: “Tragedies tend to have more streamlined plots, or less plot—you know, fewer things happening. Comedies tend to enjoy a multiplication of characters, disguises, and trickeries. I mean, you could partly think about the way [tragedies tend to move] towards the isolation of a single figure on the stage, getting rid of other people, moving towards a kind of solitude, whereas comedies tend to end with a big scene at the end where everybody’s on stage” (&lt;a href=&quot;http://podcasts.ox.ac.uk/revengers-tragedy-thomas-middleton&quot;&gt;6:02-6:37&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The distinction Smith draws between tragedies and comedies is fairly intuitive: tragedies isolate the poor player that struts and frets his hour upon the stage and then is heard no more. Comedies, on the other hand, &lt;i&gt;aggregate&lt;/i&gt; characters in order to facilitate comedic trickery and tidy marriage plots. While this discrepancy seemed promising, I couldn’t help but wonder whether computational analysis would bear out the hypothesis. Inspired by the recent proliferation of computer-assisted genre classifications of Shakespeare’s plays—many of which are founded upon &lt;a href=&quot;http://winedarksea.org/?p=40&quot;&gt;h&lt;/a&gt;&lt;a href=&quot;http://extra.shu.ac.uk/emls/09-3/hopewhit.htm&quot;&gt;i&lt;/a&gt;&lt;a href=&quot;https://www.academia.edu/8108337/Shakespeare_by_the_numbers_on_the_linguistic_texture_of_the_Late_Plays&quot;&gt;g&lt;/a&gt;&lt;a href=&quot;http://mcpress.media-commons.org/ShakespeareQuarterly_NewMedia/hope-witmore-the-hundredth-psalm/&quot;&gt;h&lt;/a&gt; &lt;a href=&quot;http://abasu.net/blog/decompiling-shakespeare.html&quot;&gt;d&lt;/a&gt;&lt;a href=&quot;http://shakespearequarterly.folger.edu/openreview/?page_id=62&quot;&gt;i&lt;/a&gt;&lt;a href=&quot;http://www.matthewjockers.net/2009/02/13/machine-classifying-novels-and-plays-by-genre/&quot;&gt;m&lt;/a&gt;&lt;a href=&quot;http://winedarksea.org/?p=1192&quot;&gt;e&lt;/a&gt;&lt;a href=&quot;http://winedarksea.org/?p=1799&quot;&gt;n&lt;/a&gt;&lt;a href=&quot;https://allistrue.wordpress.com/2010/09/29/this-thing-of-darkness-part-iii/&quot;&gt;s&lt;/a&gt;&lt;a href=&quot;https://allistrue.wordpress.com/2010/09/29/this-thing-of-darkness-part-iii/&quot;&gt;i&lt;/a&gt;&lt;a href=&quot;http://ullyot.ucalgaryblogs.ca/2012/09/07/data-curation-in-the-networked-humanities/&quot;&gt;o&lt;/a&gt;&lt;a href=&quot;http://litlab.stanford.edu/LiteraryLabPamphlet1.pdf&quot;&gt;n&lt;/a&gt;&lt;a href=&quot;http://www.mdpi.com/2076-0760/4/3/758&quot;&gt;a&lt;/a&gt;&lt;a href=&quot;http://winedarksea.org/?p=600&quot;&gt;l&lt;/a&gt; data sets like those generated by &lt;a href=&quot;http://www.cmu.edu/hss/english/research/docuscope.html&quot;&gt;DocuScope&lt;/a&gt;—I was curious to know if paying attention to the number of characters on stage in Shakespearean drama could help provide additional feature sets with which to carry out this task.&lt;/p&gt;

&lt;p&gt;To pursue the question, I ran some analysis on the Folger Digital Texts edition of the Bard’s plays. This delightful collection uses a custom XML schema to indicate when characters enter and exit the stage, which makes it possible to track the number of characters on stage over the course of a play:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/tempest.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This visualization of &lt;i&gt;The Tempest&lt;/i&gt;, for instance, traces the number of characters on stage from the play’s opening scene—in which the Shipmaster and his Boatswain are quickly joined by Alonso, Sebastian, Antonio, Ferdinand, and Ganzalo—through Prospero’s staff-dashing monologue around the 15,000 word mark to the play’s crowded conclusion. Here are the stagings for the other Shakespearean plays in the FDT canon, ordered by their date of first performance according to Alfred Harbage’s &lt;i&gt;Annals of English Drama&lt;/i&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/chars_on_stage_all.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These plots afford ample evidence to suggest that Shakespearean comedies tend to end with large scenes in which everybody’s on stage. Unfortunately, many of the comedies and tragedies also end with large gatherings of characters. It therefore seems that the number of characters on stage during a play’s conclusion might not be an ideal feature with which to classify the genres of Shakespeare’s plays.&lt;/p&gt;

&lt;p&gt;With these results in hand, I decided to measure how often Shakespeare isolates a single character on stage within plays from each of the three canonical genres. Aggregating the total number of words spoken when only a single character is on stage, as well as the total number of words spoken when only two characters are on stage, and so forth, allows one to measure the degree to which each play distributes its attention between large and small gatherings of characters (click to enlarge):&lt;/p&gt;

&lt;div class=&quot;inline-trio&quot;&gt;
  &lt;a class=&quot;img-link-wrapper&quot; href=&quot;/images/post_images/classifying_shakespearean_drama/comedies.png&quot; data-lightbox=&quot;genre-characters&quot; data-title=&quot;&quot;&gt;
    &lt;img class=&quot;img-link&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/comedies.png&quot; alt=&quot;Comedies&quot; /&gt;
  &lt;/a&gt;

  &lt;a class=&quot;img-link-wrapper&quot; href=&quot;/images/post_images/classifying_shakespearean_drama/histories.png&quot; data-lightbox=&quot;genre-characters&quot; data-title=&quot;&quot;&gt;
    &lt;img class=&quot;img-link&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/histories.png&quot; alt=&quot;Histories&quot; /&gt;
  &lt;/a&gt;

  &lt;a class=&quot;img-link-wrapper&quot; href=&quot;/images/post_images/classifying_shakespearean_drama/tragedies.png&quot; data-lightbox=&quot;genre-characters&quot; data-title=&quot;&quot;&gt;
    &lt;img class=&quot;img-link&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/tragedies.png&quot; alt=&quot;Tragedies&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;While these plots reveal some interesting features of the works, such as the fact that &lt;i&gt;Two Gentlemen of Verona&lt;/i&gt; truly does revolve around dyadic pairs, they make it difficult to compare the amount of time tragedies and comedies feature only a single character on stage. To make this latter comparison, one can find the average amount of time a single character occupies the stage for each genre:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/means_by_genre.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Surprisingly, the chief difference between the comedies and tragedies has less to do with the way each handles isolated actors on stage than with the way each handles triads and quadrads. It seems tragedies have a greater tendency to revolve around sets of three characters, while comedies are more often organized around sets of four characters. That said, the similarities between the two genres are far more striking than their differences, and far less encouraging for one in search of distinguishing features.&lt;/p&gt;

&lt;p&gt;Reflecting on these results, I wondered if tragedies might be better classified by the amount of time their conflicted characters spend addressing the audience. One way to begin measuring the latter, I thought, would be to count the number of words spoken by each character in each play (click to enlarge):&lt;/p&gt;

&lt;div class=&quot;inline-trio&quot;&gt;
  &lt;a class=&quot;img-link-wrapper&quot; href=&quot;/images/post_images/classifying_shakespearean_drama/speakers_comedies.png&quot; data-lightbox=&quot;genre-speakers&quot; data-title=&quot;&quot;&gt;
    &lt;img class=&quot;img-link&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/speakers_comedies.png&quot; alt=&quot;Comedies&quot; /&gt;
  &lt;/a&gt;

  &lt;a class=&quot;img-link-wrapper&quot; href=&quot;/images/post_images/classifying_shakespearean_drama/speakers_histories.png&quot; data-lightbox=&quot;genre-speakers&quot; data-title=&quot;&quot;&gt;
    &lt;img class=&quot;img-link&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/speakers_histories.png&quot; alt=&quot;Histories&quot; /&gt;
  &lt;/a&gt;

  &lt;a class=&quot;img-link-wrapper&quot; href=&quot;/images/post_images/classifying_shakespearean_drama/speakers_tragedies.png&quot; data-lightbox=&quot;genre-speakers&quot; data-title=&quot;&quot;&gt;
    &lt;img class=&quot;img-link&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/speakers_tragedies.png&quot; alt=&quot;Tragedies&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Analyzing these figures, I was struck by what should have been a fairly obvious fact: Shakespeare’s most memorable characters (Falstaff, Hal, Prospero, Rosalind, Hamlet…) are each given commanding positions within the plays they lead. Given the strong correlation between these memorable characters and the number of lines each speaks, it’s tempting to ask whether we remember these characters most readily simply because Shakespeare allowed them to say the most, or whether Shakespeare allowed them to say the most because he sensed they were his most memorable characters.&lt;/p&gt;

&lt;p&gt;Either way, the last trio of plots shows a fairly even distribution of commanding figures among the comedies, histories, and tragedies. But those plots also reveal that the histories include rather few words spoken by women, as well as the fact that the comedies tend to be shorter than the tragedies and histories:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/classifying_shakespearean_drama/female_presence_vs_length.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By analyzing only the length of a play and the number of words women speak in that play, one can start to get reasonably good separation between the genres: comedies tend to be shorter and include more female dialogue, histories tend to be longer and include less female dialogue, and tragedies split provocatively between the upper right and lower left. Reviewing these figures, I can’t shake the suspicion that a third dimension of data could unite these divided tragedies. But what would that dimension consist of?&lt;/p&gt;

&lt;div class=&quot;center-text&quot;&gt;* * *&lt;/div&gt;

&lt;p&gt;I would like to thank Mike Poston, co-curator of the Folger Digital Text editions used for this analysis, for discussing many of the finer points of the FDT collection with me. In case you want to replicate any of the analysis or assess the assumptions on which it’s founded, the scripts are &lt;a href=&quot;https://github.com/duhaime/mining_the_bard&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Sun, 12 Oct 2014 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/classifying-shakespearean-drama-with-sparse-feature-sets</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/classifying-shakespearean-drama-with-sparse-feature-sets</guid>
        
        
        <category>digital-humanities</category>
        
        <category>shakespeare</category>
        
      </item>
    
      <item>
        <title>Pseudo-Cryptography in Jonathan Swift&#39;s Tale of a Tub</title>
        <description>&lt;p&gt;“I do here humbly propose for an Experiment,” Jonathan Swift writes near the end of his &lt;i&gt;Tale of a Tub&lt;/i&gt;, “that every Prince in Christendom will take seven of the deepest Scholars in his Dominions, and shut them up close for seven Years, in seven Chambers, with a Command to write seven ample Commentaries upon [&lt;i&gt;A Tale of a Tub&lt;/i&gt;].” In order to promote so useful a Work, Swift informs readers that he has encrypted some hidden messages in the &lt;i&gt;Tale&lt;/i&gt;: “I have couched a very profound Mystery in the Number of O’s multiply’d by Seven, and divided by Nine.” Not wishing to leave the alchemically inclined out of his game, Swift adds: “Also, if a devout Brother of the Rosy Cross will pray fervently for sixty three Mornings, with a lively Faith, and then transpose certain Letters and Syllables according to Prescription, in the second and fifth Section; they will certainly reveal into a full Receipt of the &lt;i&gt;Opus Magnum&lt;/i&gt;.” Swift then completes his triumvirate of puzzles with the following remark: “Lastly, Whoever will be at the Pains to calculate the whole Number of each Letter in this Treatise, and sum up the Difference exactly between the several Numbers, assigning the true natural Cause for every such Difference; the Discoveries in the Product, will plentifully reward his Labour” (Section X).&lt;/p&gt;

&lt;p&gt;The probability that Swift actually altered his language so as to conceal occult knowledge in his already-overstuffed &lt;i&gt;Tale&lt;/i&gt; may seem rather minimal to many readers. For those familiar with Swift’s delight in word games and ciphers, though, the odds might look a bit brighter. In fact, readers who have discovered Paul Childs’ essay “Cipher Against Ciphers: Jonathan Swift’s Latino-Anglicus Satire of Medicine” in &lt;i&gt;Cryptologia&lt;/i&gt;—which analyzes the ways Swift transformed his suffering under Ménière’s disease into cleverly encrypted messages—might wonder whether the Dean has in fact buried some treasure in his &lt;i&gt;Tale&lt;/i&gt;. With this question in mind, I decided to run some experiments to see whether I might be able to resolve some of Swift’s long-overlooked riddles.&lt;/p&gt;

&lt;p&gt;Riddle One: The Mystery of the Number of O’s in Swift’s &lt;i&gt;Tale&lt;/i&gt;. Some simple analysis reveals that there are 16,092 O’s in the 1710 edition of Swift’s &lt;i&gt;Tale of a Tub&lt;/i&gt;. If we multiply this value by seven and divide the result by nine—the operations Swift suggests one must perform to uncover his profound Mystery—we get 12,516, a number whose significance I leave to others to determine. While this number might carry significance, the more fundamental question is whether Swift’s use of the letter O appears to be unusual or premeditated in any way. To answer this question, we can compare the relative frequency of the letter O in Swift’s Tale to the relative frequency of the letter in other documents from the eighteenth century:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/pseudo_cryptography/relative_frequency_o.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If Swift’s usage of the letter O were premeditated or unusual in any way, we should expect to see the relative frequency of the letter depart from the norm established by his contemporaries. As the plot above indicates, however, his use of the letter is perfectly in keeping with the trend of his times, which suggests Swift’s first riddle is a jest.&lt;/p&gt;

&lt;p&gt;Riddle Two: “[If readers] transpose certain Letters and Syllables according to Prescription, in the second and fifth Section; they will certainly reveal into a full Receipt of the Opus Magnum”. To analyze this puzzle, we can again look at the distribution of letters in Swift’s &lt;i&gt;Tale&lt;/i&gt;, this time investigating the degree to which the distribution of any letter in sections two and five look out of keeping with the distributions of those letters in other sections. More generally, we can look to see if there are any unusual distributions of letters across the sections of the text, and if there are, we can begin considering appropriate methods of transposing those letters to get the syllables with which the &lt;i&gt;Magnum Opus&lt;/i&gt; is communicated.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/pseudo_cryptography/tale_letter_distributions.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each section of the &lt;i&gt;Tale&lt;/i&gt; is given a consistent color in this plot, so if any section contains an unusual proportion of any particular letter(s), we should expect to see a wider distribution of frequencies for that letter or those letters. Much to the would-be alchemist’s chagrin, however, the plot above indicates that there are no letters in the Tale that have wildly aberrant distributions, which effectively closes the book on the second riddle.&lt;/p&gt;

&lt;p&gt;Riddle Three: “Whoever will . . . calculate the whole Number of each Letter in this Treatise, and sum up the Difference exactly between the several Numbers, assigning the true natural Cause for every such Difference; the Discoveries in the Product, will plentifully reward his Labour.” We can easily calculate the number of times each letter occurs in Swift’s &lt;i&gt;Tale&lt;/i&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/pseudo_cryptography/tale_raw_letter_freqs.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using these frequencies, Swift suggests, one can find “the Discoveries in the Product,” which value “will plentifully reward his Labour.” He leaves it comically unclear what is meant by “the Product,” though: does he mean the product of the difference between each letter and each other letter, or the product between the difference of each letter and the “whole Number of each Letter in this Treatise”, or some other mad metric?&lt;/p&gt;

&lt;p&gt;Regardless of the answer to this question, it seems clear that Swift means these riddles to be ludic and satirical, rather than genuine encryptions. My question for readers is: who or what is Swift parodying here? Are there other texts from the period that purport to contain hidden messages in their letter counts? I ask not only because I’m fascinated by early modern ciphers, but because I want to have a fuller understanding of the specific works with which Swift was working in these delightfully flippant passages.&lt;/p&gt;

&lt;div class=&quot;center-text&quot;&gt;* * *&lt;/div&gt;

&lt;p&gt;The analysis above was conducted using the &lt;a href=&quot;http://www.lehigh.edu/~amsp/tubb0-0.html&quot;&gt;1710 edition&lt;/a&gt; of the Tale digitized by Lehigh University.  The visualizations were produced with &lt;a href=&quot;http://matplotlib.org/api/pyplot_api.html&quot;&gt;Pyplot&lt;/a&gt; using &lt;a href=&quot;/assets/scripts/posts/pseudo_cryptography/swift_tale_scripts.zip&quot;&gt;these scripts&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Wed, 06 Aug 2014 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/pseudo-cryptography</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/pseudo-cryptography</guid>
        
        
        <category>digital-humanities</category>
        
        <category>cryptography</category>
        
      </item>
    
      <item>
        <title>Co-citation Networks in the EEBO TCP</title>
        <description>&lt;p&gt;I recently had the good fortune of attending a conference on &lt;a href=&quot;http://www.notwithoutmustard.net/beyond-authorship/&quot;&gt;computational approaches to early modern literature&lt;/a&gt; hosted at the University of Newcastle. During the conference, I not only got to meet some outstanding scholars—including Doug Bruster, John Burrows, Hugh Craig, Mac Jackson, and Glenn Roe, to name only a few—but I also had a chance to present some of my recent work on algorithmic approaches to the study of literary influence. In case it might be of interest to others working in related fields, I thought I would share one of the approaches I discussed in what follows below.&lt;/p&gt;

&lt;p&gt;Buried within the EEBO-TCP corpus, I learned some months ago, is a veritable trove of metadata. These [metadata features] indicate when authors include things like stage directions, tables of data, and alchemical symbols in their writing, all of which is great news for the computationally inclined. For those interested in influence, there are also metadata fields that indicate when an author is quoting or citing another text. By placing &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;q&amp;gt;&lt;/code&gt; tags around quotations and &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;bibl&amp;gt;&lt;/code&gt; tags around citations of authors or works, the Text Creation Partnership made it easy for researchers to begin looking for citational patterns in early modern literature:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/eebo_cocitation/sample_eebo_xml.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using a simple script, one can easily extract all of the quotations and bibliographic citations from all files in the EEBO-TCP corpus. Because the EEBO-TCP corpus contains roughly one third of the titles from the period recorded in the definitive English Short Title Catalogue, this citational data can serve as a fairly representative archive of intertextual trends in early modern England:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/eebo_cocitation/estc_eebo-tcp_titles.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That is, at least, the idea in theory. In practice, the data is quite messy, and in its native form, is all but algorithmically unapproachable. Take, for example, the following references contained within &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;bibl&amp;gt;&lt;/code&gt; brackets:&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;IUDGES V. XXIII.
Iudges, 5 23.
IVDGES 4. 21.
Iudg. 42.
[Judg. 21.]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Human investigators who look into the matter can easily discover that each of these references refers to the Book of Judges. To allow computers to recognize this fact, however, I had to spend several weeks sifting my way through the collection of &lt;bibl&gt; tags, carefully identifying the texts and authors to which those metadata fields referred. In the end, of the roughly 45,000 items that were tagged as references to books or authors in the EEBO-TCP corpus (parts I and II), I found roughly a third to be too cryptic to decipher:&lt;/bibl&gt;&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;W. ??.
Qu.
ﾗ Testimony of a great Divine.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Setting these obscure references aside, I cleaned up the sources about which I was reasonably confident, such as the references to Judges above. After I had aggregated all of these, I was naturally curious to see which texts and authors were most cited in the corpus. Here are the top forty:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/eebo_cocitation/eebo_bibl_top_forty.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Biblical citations predominate, with the greatest number of references going to the Book of Psalms. (It should be noted that in most translations of the Bible into English and Latin, the Book of Psalms has by far the greatest number of verses and words.) Matthew leads the Evangelists, followed by John, then Luke, then Mark. The highest frequency Greco-Roman writers include Virgil, Ovid, Horace, Martial, Juvenal, and Seneca, in that order. While this data might not revolutionize early modern scholarship, it might be helpful in the classroom. When teaching undergraduates about the works early modern audiences read, heard, and cited, for example, such visualizations can help to bring home the profound religiosity of the age.&lt;/p&gt;

&lt;p&gt;Drawing on the same data that underlies this visualization, one can also analyze networks of influence in early modern literature. Rather than conceive of influence as a series of vectors, each measuring the number of times a given work or author is cited in the TCP corpus, one can analyze the kinds of works and authors that are cited together. Such forms of analysis are often referred to as “co-citation networks,” or networks of references that tend to be cited together. For example, say our corpus contained references to only nine different sources:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/eebo_cocitation/simple_sample_network.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In that case, we could visualize each of these sources as a “node” in a network graph and could create an “edge,” or connecting line, between any two nodes that were cited within a text in the corpus. The sample graph above would then illustrate the fact that references “A” and “B” are cited within one text, “G” and “Z” in another, and so on. Using this method on the EEBO-TCP corpus, I thought, might help to reveal latent structures embedded within early modern citation networks. With the curated EEBO-TCP citation data in hand, I therefore proceeded in the fashion described above, transforming each cited author or work into a node, and creating edges between any two nodes cited within a single work. Here is one visualization of the results:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/eebo_cocitation/eebo_co-citation.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this graph, each node has been assigned a color. These colors are determined by an algorithm that identifies clusters of nodes that are commonly cited together, and then colors the different clusters accordingly. Analyzing the nodes that cluster together, one finds four particularly well-defined groups of references: the light blue cluster of nodes, which are biblical books (Leviticus, Corinthians); the red cluster of nodes, which are classical references (Macrobius, Lucretius); the yellow cluster of nodes, which are Reformation-era martyrs and theologians (Theodore Beza, Richard Turner); and the purple cluster of nodes, which are literary writers (Philip Sidney, William Shakespeare). The results of the procedure are intuitively legible: each group represents a fairly homogenous collection of authors and works, and the divisions that split the groups apart represent fairly significant generic differences between the various references contained in the data. If this is right, then such a map could perhaps serve as a useful resource in the classroom. When discussing the so-called “battle of ancients and moderns,” for instance, graphs like the present one might help students visualize the different ways early modern citation practices established divisions between these groups.&lt;/p&gt;

&lt;p&gt;Using a slightly different visualization, one can identify the points of connection between and among these ostensibly divided groups. &lt;a href=&quot;http://duhaime.github.io/eebo-cocitation/&quot;&gt;This link&lt;/a&gt; will take you to an interactive site that contains all of the data included in the previous plot, including the modularity rankings that separated and color-coded the nodes. Unlike the previous plot, however, the visualization at the other end of that link allows users to click on a particular node and see all of the others to which that node is connected. Comparing the networks of each of these nodes, one can produce at a glance one measure of the degree to which the given node was cited with works and authors from the classical inheritance, for instance. The results sometimes lead to new questions. For example, the interactive plot demonstrates that when early modern authors cited Paul the Apostle in the context of classical writers like Apuleius, they cited Paul’s given name; when they cited Paul in biblical contexts, by contrast, they cited his works, such as “Romans.” Why might that be the case? Other points of interest I’ve come across in this visualization include the relatively isolation of the alchemists (George Ripley, Geber, etc.), towards the bottom of the plot, and the predominance of biblical citations in works that reference Descartes. Both of these observations strike me as less intuitive than the separation of authors into disparate modularity rankings, for instance, and both seem worthy of further inquiry.&lt;/p&gt;

</description>
        <pubDate>Thu, 26 Jun 2014 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/cocitation-networks-in-the-eebo-tcp</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/cocitation-networks-in-the-eebo-tcp</guid>
        
        
        <category>digital-humanities</category>
        
        <category>networks</category>
        
      </item>
    
      <item>
        <title>Batch Processing Python Scripts on Sun Grid Engine Queues</title>
        <description>&lt;p&gt;Suppose you have a collection of text files and would like to compare each of those files to each of the others. Perhaps you would like to know which characters, locations, or stage directions in each file occur in any of the others. Whatever the task, if your collection is small enough—on the order of a few paragraphs, say—you can of course compare the files manually, reading each of your paragraphs in turn, and comparing the given paragraph to each of the others. If your collection is a bit bigger—on the order of a few hundred novels, say—you might automate these comparisons on your computer. If your collection is really big, however, a single computer might not be powerful enough to finish the job during your lifetime.&lt;/p&gt;

&lt;p&gt;Comparing each file in a four text corpus to each of the others, after all, only involves six comparisons. Running the same analysis on a collection of 50,000 files (roughly the size of the Project Gutenberg collection in English, or the EEBO-TCP corpus), however, means running 1,249,975,000 comparisons. If each of those comparisons takes one minute to execute on your computer, it will take 2376 years to run this job on your machine. Thankfully, we can expedite this process tremendously by leveraging the power of distributed computing systems like Sun Grid Engine (SGE) queues. Pursuing the routine described above, for instance, we can use an SGE system to run each of our 1+ billion comparisons in a few minutes.&lt;/p&gt;

&lt;p&gt;To get started, we’ll want to create an “iteration schedule” in which we identify all of the comparisons we wish to run. Here is a visual representation of an iteration schedule for a four text corpus:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/batch_processing_on_sge/iteration_schedule_map.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the table above, each of our iterations-to-be-run is denoted by an “o.” Each “o” sits in the cell that joins the row and the column that denote the two texts to be analyzed in the given iteration. Reading across our first row, for instance, we see that text one does not need to be compared to text one, but does need to be compared to texts two, three, and four. The second row denotes that we want to compare text two to texts three and four, and row three denotes that we want to compare text three to text four.&lt;/p&gt;

&lt;p&gt;After determining all of the comparisons to be run, we will want to render that information in machine-readable form. More specifically, we want to generate a table that has three columns: iteration_number, first_text, and second_text: where first_text and second_text are the file names of the two texts we wish to compare in the given iteration, and iteration_number is an integer whose value is zero in the first row of the iteration schedule, 1 in the next row, 2 in the next, . . . and n in the last, where n equals the total number of comparisons we wish to make. [In general, the number of comparisons required is (p-1)(p)/2, where p equals the number of files in your corpus.]  Here is a sample iteration table (produced by this [script][script-one]):&lt;/p&gt;

&lt;div&gt;
  &lt;pre data-line=&quot;&quot;&gt;&lt;code class=&quot;language-shell&quot;&gt;0 A00002.txt A00005.txt
1 A00002.txt A00007.txt
2 A00002.txt A00008.txt
3 A00002.txt A00011.txt
4 A00002.txt A00012.txt
5 A00002.txt A00013.txt
6 A00002.txt A00014.txt
7 A00002.txt A00015.txt
8 A00002.txt A00018.txt
9 A00002.txt A00019.txt&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you want to batch process a different kind of routine on an SGE system, you can modify your iteration schedule appropriately. If you only want to calculate the type-token ratios of each of your files, for instance, you’ll only need two columns: iteration_number and text_name. Once this iteration schedule is all set, we can turn to the script to be run during each of these iterations. &lt;a href=&quot;https://github.com/duhaime/batch_processing_on_sge/blob/master/test.py&quot;&gt;Here’s mine&lt;/a&gt;: If you want to run a different kind of analysis, just keep lines 1-21 and line 102 of that script, and use the variable “iteration_number” to guide which texts you will analyze in each iteration. Once your routine is all set, save it as “test.py” and upload it—along with your iteration schedule, the files you wish to compare, and these two &lt;a href=&quot;https://github.com/duhaime/batch_processing_on_sge/blob/master/_run_me.py&quot;&gt;higher order scripts&lt;/a&gt;—to a single directory on your SGE server:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/batch_processing_on_sge/prepared_for_batch_submission.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once all of these files are in the same directory, you are ready to submit your script for batch processing. To do so using the University of Notre Dame’s Center for Research Computing system, you can simply type “python _run_me.py cmd_run.py your_netid” with no quotation marks:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/batch_processing_on_sge/batch_sge_submission.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After you submit this command, the higher-order Python scripts &lt;em&gt;run_me.py and cmd_run.py will create new copies of your test.py script, changing the input files for each iteration according to your iteration schedule. If all has gone well, and you refresh the directory after a few moments, you’ll see a few (or more than a few, depending on the number of iterations you are running!) new files in your directory. More specifically, you’ll have a collection of new job&lt;/em&gt; files that give you feedback on the result of each of your iterations. If errors cropped up during your analysis, those errors should be recorded in these job_ files. Provided that there were no exceptions, though, those files will be empty, and you will find in your directory whatever output you requested in your test.py script. Et voila, now you can finish your analysis in a few minutes, rather than a few millennia!&lt;/p&gt;

&lt;div class=&quot;center-text&quot;&gt;* * *&lt;/div&gt;

&lt;p&gt;I want to thank Scott Hampton and Dodi Heryadi of Notre Dame’s High Performance Computing Group, who helped me think through the logistics of batch processing, Reid Johnson of Notre Dame’s Computer Science Department, who sent me the higher-order SGE scripts on which this analysis is based, and Tim Peters, who wrote many of the Python modules on which my work depends!&lt;/p&gt;

</description>
        <pubDate>Thu, 26 Jun 2014 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/batch-processing-python-scripts-on-sge-queues</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/batch-processing-python-scripts-on-sge-queues</guid>
        
        
        <category>python</category>
        
        <category>hpc</category>
        
      </item>
    
      <item>
        <title>Identifying Poetry in Unstructured Corpora</title>
        <description>&lt;p&gt;Over the last few months, I’ve been working with colleagues at Notre Dame to develop computational approaches we can use to identify the genres to which a literary work belongs. Initially, we focused our research on the georgic, a class of agricultural-cum-labour poems that flourished in the seventeenth and eighteenth centuries. Eventually, though, our limited research corpus led us to investigate methods we could use to identify more period poetry, and these investigations helped reveal a fascinating if simple method one can use to identify poetic works in unstructured corpora.&lt;/p&gt;

&lt;p&gt;We began building our corpus of early modern English poetry by identifying the poetry curated by the Text Creation Partnership (TCP). Running a simple Python script over the TCP’s selections from the Early English Books (EEBO) corpus—which stretches from “the first book printed in English in 1475 through 1700”—and the Eighteenth Century Collections (ECCO) corpus, we extracted all the lines of text wrapped in &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;l&amp;gt;&lt;/code&gt; tags (the &lt;a href=&quot;http://www.tei-c.org/release/doc/tei-p5-doc/en/html/REF-ELEMENTS.html&quot;&gt;TEI designation&lt;/a&gt; for a line of verse). This left us with 16,571 text files, each of which contained only poetry from roughly the sixteenth through the eighteenth centuries. After examining some of these files, we realized that many consisted entirely of poetic epigraphs, so we used another script to remove all of these small files (those smaller than 16 kb) from our research corpus, leaving us with a fairly substantive collection of poetic works from the period of interest:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/identifying_poetry_in_unstructured_corpora/eebo_ecco_tcp_poetry.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Because the EEBO-TCP contains 44,255 volumes—roughly one third of all titles recorded in &lt;a href=&quot;http://estc.ucr.edu/ESTCStatistics.html&quot;&gt;Alain Veylit’s ESTC data&lt;/a&gt; for the appropriate years—we felt reasonably confident that our holdings for the sixteenth and seventeenth centuries were fairly representative of literary trends during the period. The ECCO-TCP, on the other hand, contains only 2,387 texts, less than one percent of ESTC titles from the eighteenth century. Even if we accept &lt;a href=&quot;http://library.oxfordjournals.org/content/s6-VIII/1/32.full.pdf&quot;&gt;John Feather’s argument&lt;/a&gt; that only 25,131 literary works were written in English during the eighteenth century—11,789 of which, he claims, were poetic works—we are left to conclude that the 1,698 files in the ECCO-TCP corpus that contain poetry might not be indicative of poetic trends from the period. Given these conclusions, we were eager to supplement our collection of eighteenth-century poetry.&lt;/p&gt;

&lt;p&gt;But where on earth can one find enormous quantities of eighteenth-century poetry in digital form? (This isn’t meant to be a rhetorical question; if you’ve got ideas, please let us know!) After considering the issue for some time, we elected to work with Project Gutenberg. Unfortunately, only after we had &lt;a href=&quot;http://webapps.stackexchange.com/questions/12311/how-to-download-all-english-books-from-gutenberg&quot;&gt;downloaded&lt;/a&gt; and unzipped all of the English files on Project Gutenberg did we realize that the enormous text collection (roughly 45,000 volumes) is all but entirely unstructured. We couldn’t find any master list of file names, author names, publication dates, or any other essential metadata fields, so we had to build our own.&lt;/p&gt;

&lt;p&gt;In the first place, we wanted to be able to differentiate poetic texts from non-poetic texts. While I imagine it would be possible to complete this task by analyzing the relative frequency of strings from each of these texts in the manner described in the previous post, we didn’t have reliable publication dates for the Gutenberg texts, so we needed an alternative method. Operating on the hypothesis that poetic texts have more line breaks and fewer words per line than prose works, we decided to measure the number of words in each line of each file. We then collected a random sample of poetic works to see what their words-per-line profiles looked like:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/identifying_poetry_in_unstructured_corpora/gutenberg_poetry.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In these plots—each of which represents a single poetic text—the numbers along the x-axis indicate the number of words in a line of the text file, and the y-axis indicates the relative frequency of lines that contain such-and-such a number of words within the text. In &lt;i&gt;The Poetical Works of James Beattie&lt;/i&gt;, for instance, only ~5% of lines had 12 or 13 words in them, whereas almost 20% of the text’s lines had 7 or 8 words in them. In other words, &lt;i&gt;The Poetical Works of James Beattie&lt;/i&gt; is dominated by lines with seven or eight words in them, a fact that applies to all of the poetic works plotted above. With these figures in hand, we plotted the words-per-line profiles for a random assortment of prose works from roughly the same period:&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;center-image&quot; src=&quot;/images/post_images/identifying_poetry_in_unstructured_corpora/gutenberg_prose.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We were pleased to see that these plots differed from the poetic plots quite dramatically! Comparing the two sets of curves, we see that poetic works contain a preponderance of lines with 7-8 words, while prose works contain a preponderance of lines with 11-12 words. This is naturally due to the fact that lines of text in prose works run across an entire page, while poets break lines strategically (and regularly in eighteenth-century verse). To identify poetry in unstructured corpora, then, we can calculate a text’s words-per-line profile, and use the results of those calculations in order to classify each text in our corpus as a work of poetry or a work of prose. Using a rather simple approach to the latter task, we found 3150 poetic works tucked in the Gutenberg corpus, a few hundred of which are from our period and can thus contribute to our study of genre classification.&lt;/p&gt;

</description>
        <pubDate>Mon, 05 May 2014 11:24:24 -0400</pubDate>
        <link>http://yourdomain.com/posts/identifying-poetry-in-unstructured-corpora</link>
        <guid isPermaLink="true">http://yourdomain.com/posts/identifying-poetry-in-unstructured-corpora</guid>
        
        
        <category>digital-humanities</category>
        
        <category>poetry</category>
        
      </item>
    
  </channel>
</rss>
